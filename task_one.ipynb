{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from netCDF4 import Dataset\n",
    "from rasterio import Affine as A\n",
    "import numpy as np\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, basename, exists\n",
    "import xarray\n",
    "from rasterio.transform import from_origin\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the directory and the program will parse all netcdf geodata\n",
    "dir_path = input(\"[*] Please enter the directory path to the file: \")\n",
    "save_folder_path = join(dir_path, \"upscale\")\n",
    "gfed5_variable_names = [\"Crop\", \"Defo\", \"Peat\", \"Total\"]\n",
    "files =  [\n",
    "    join(dir_path, file)\n",
    "    for file in listdir(dir_path)\n",
    "    if isfile(join(dir_path, file))\n",
    "    and (file.split(\".\")[-1] == \"hdf5\" or file.split(\".\")[-1] == \"nc\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_matrix_numpy(source_matrix, window_height = 8, window_width = 10):\n",
    "    \"\"\"\n",
    "    Function preforms the process of upscaling the passed in matrix using numpy or skimage\n",
    "    (first determining if this is possible and then preforming the operation)\n",
    "    \n",
    "    :param source_matrix: matrix we wish to compress (upscale)\n",
    "    :param window_height: height of the window we wish to iterate over with\n",
    "    :param window_width: width of the window we wish to iterate over with\n",
    "    :return: upscaled matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        source_shape = source_matrix.shape   \n",
    "        # check if the window size lines up evenly with the passed in matrix\n",
    "        if (source_shape[0] % window_height == 0) and (source_shape[1] % window_width == 0):\n",
    "            # This is another method to reduce a matrix with a window using a sum calculation (both work the same)\n",
    "            # downscaled_data = block_reduce(source_matrix, block_size=(window_height, window_width), func=np.sum)\n",
    "            # reshape the matrix into a 4D matrix (shows each window of the matrix)\n",
    "            reshape_result = source_matrix.reshape(source_shape[0] // window_height, window_height, source_shape[1] // window_width, window_width)\n",
    "            # sum the windows and creates the 2D matrix\n",
    "            result = reshape_result.sum(axis=(1, 3))\n",
    "            print(\"[+] Successfully upscaled matrix, current updated matrix shape: \", np.asarray(result).shape)\n",
    "            return result\n",
    "    except Exception as error:\n",
    "        print(\"[-] Failed to upscale matrix\", error)\n",
    "        return source_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geotiff_file(data_arr, latitude_arr, longitude_arr):\n",
    "    # obtain the data_arr shape\n",
    "    height, width = data_arr.shape\n",
    "    # create a transformation of the data to match a global map\n",
    "    transform = from_origin(\n",
    "        longitude_arr[0],\n",
    "        latitude_arr[-1],\n",
    "        abs(longitude_arr[1] - longitude_arr[0]),\n",
    "        abs(latitude_arr[-1] - latitude_arr[-2]),\n",
    "    )\n",
    "    \n",
    "    # outline meta data about the geotiff file\n",
    "    metadata = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"crs\": \"EPSG:3857\", # optional formats EPSG:3857 (works on panoply) EPSG:4326 (works well on leaflet)\n",
    "        \"transform\": transform,\n",
    "    }\n",
    "\n",
    "    # obtain the GeoTIFF path\n",
    "    geotiff_file_path = join(save_folder_path, \"output.tif\")\n",
    "    # Create a new GeoTIFF file using the crafted path and add the data to the file \n",
    "    with rasterio.open(geotiff_file_path, \"w\", **metadata) as dst:\n",
    "        # total_data_value = np.flip(data_arr, 0)\n",
    "        dst.write(data_arr, 1)\n",
    "    # return the GeoTIFF file path\n",
    "    return geotiff_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_matrix_restario(source_matrix, dest_dimensions):\n",
    "        \"\"\"\n",
    "        Function preforms the process of upscaling the passed in matrix using rasterio\n",
    "        Issues - There is no errors however the result produces an array matching the desired dimensions but missing any values\n",
    "\n",
    "        :param source_matrix: matrix we wish to compress (upscale)\n",
    "        :param dest_dimensions: a tuple/list containing the dimensions you want to transform the matrix\n",
    "        :return: reshaped numpy matrix\n",
    "        \"\"\"\n",
    "        # https://github.com/corteva/rioxarray/discussions/332\n",
    "        \n",
    "        #Obtain the numpy array shape\n",
    "        height, width = source_matrix.shape\n",
    "        # create a long and latitude numpy array\n",
    "        latitude_arr = np.linspace(-90, 90, height)\n",
    "        longitude_arr = np.linspace(-180, 180, width)\n",
    "        \n",
    "        # create the geotiff file and return the path to that file\n",
    "        geotiff_file_path = create_geotiff_file(\n",
    "            source_matrix, latitude_arr, longitude_arr\n",
    "        )\n",
    "        \n",
    "        #open that newly created geotiff file\n",
    "        raster = riox.open_rasterio(geotiff_file_path)\n",
    "\n",
    "        # preform upsampling using rasterio and rioxarray\n",
    "        up_sampled = raster.rio.reproject(\n",
    "            raster.rio.crs,\n",
    "            shape=(int(dest_dimensions[0]), int(dest_dimensions[1])),\n",
    "            resampling=rasterio.warp.Resampling.sum,\n",
    "        )\n",
    "        \n",
    "        # obtain the data\n",
    "        data_value = up_sampled.values[0]\n",
    "        # close the file (script will yell at you if you dont)\n",
    "        raster.close()\n",
    "        # return numpy data array\n",
    "        return data_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_upscale_sum(origin_matrix, upscaled_matrix, margin_of_error = 65536.0):\n",
    "    \"\"\"\n",
    "    Function prints our the original matrix sum and the upscaled matrix sum (post re grid)\n",
    "    It then determines if the upscaled matrix sum is close enough to the original matrix sum (incorporating a margin of error)\n",
    "    \n",
    "    :param origin_matrix: original matrix before re grid\n",
    "    :param upscaled_matrix: the original matrix post re grid\n",
    "    :param margin_of_error: the margin of error the allowed \n",
    "    :return: boolean\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(f\"Original Burned Area Total - {origin_matrix.sum()}\")\n",
    "    print(f\"\\tOriginal Burned Area Dimensions - {origin_matrix.shape}\")\n",
    "    print(f\"\\tOriginal Burned Area Mean - {origin_matrix.mean()}\")\n",
    "    print(f\"Upscale Burned Area Total - {upscaled_matrix.sum()}\")\n",
    "    print(f\"\\tUpscale Burned Area Dimensions - {upscaled_matrix.shape}\")\n",
    "    print(f\"\\tUpscale Burned Area Mean - {origin_matrix.mean()}\")\n",
    "    print()\n",
    "    \n",
    "    # returns true if the upscaled matrix sum is within the range of the original matrix sum (margin of error accounts for rounding of values)\n",
    "    return upscaled_matrix.sum() >= (origin_matrix.sum() - margin_of_error) and upscaled_matrix.sum() <= (origin_matrix.sum() + margin_of_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_new_filename(file_path) -> str:\n",
    "    # creates a file name (adding upscale to the current file name)\n",
    "    file_name = basename(file_path)\n",
    "    file_name_list = file_name.split(\".\")\n",
    "    if len(file_name_list) > 1:\n",
    "        file_name_list[-2] = file_name_list[-2] + \"(upscaled)\"\n",
    "        # ensures the file is saved as a netcdf file\n",
    "        file_name_list[-1] = \"nc\"\n",
    "        # return the rejoined list and the added classes save folder path\n",
    "        return join(save_folder_path, \".\".join(file_name_list))\n",
    "    return join(save_folder_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_path, data_set) -> None:\n",
    "    \"\"\"\n",
    "    Saves the xarray dataset based on the file inputted to the function\n",
    "\n",
    "    :param file_path: file path of the current file being upscaled/processed\n",
    "    :param data_set: data set representing the\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create the new file's path & name\n",
    "        new_file_name = obtain_new_filename(file_path)\n",
    "        # checks if the save folder path exists (if it does not a folder is created)\n",
    "        if not exists(save_folder_path):\n",
    "            makedirs(save_folder_path)\n",
    "        # saves the file using the created file path and xarray\n",
    "        data_set.to_netcdf(path=(new_file_name))\n",
    "        print(f\"[+] file {new_file_name} saved\")\n",
    "    except Exception as error:\n",
    "        print(\n",
    "            \"[-] Failed to save dataset (ensure dataset is from xarray lib): \",\n",
    "            error,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_geodata(origin_matrix, upscaled_matrix, longitude, latitude) -> None:\n",
    "    \"\"\"\n",
    "    Saves the xarray dataset based on the file inputted to the function\n",
    "    \n",
    "    :param file_path: file path of the current file being upscaled/processed \n",
    "    :param data_set: data set representing the \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # map = Basemap(projection='merc',llcrnrlon=-90.,llcrnrlat=90.,urcrnrlon=-180.,urcrnrlat=180.,resolution='i') # projection, lat/lon extents and resolution of polygons to draw\n",
    "    # lon, lat = np.meshgrid(longitude, latitude)\n",
    "    # xi, yi = map(lon, lat)\n",
    "    # resolutions: c - crude, l - low, i - intermediate, h - high, f - full\n",
    "    plt.imshow(origin_matrix, interpolation='none')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(upscaled_matrix, interpolation='none')\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loops through each file in the classes files list Regridding (upscaling) datasets from a fine resolution to a coarse (ModelE) resolution\n",
    "Note - This is focused on the burned area dataset and uses both netcdf (parsing/reading) and xarray (saving the data)\n",
    "    Issue (SOLVED) - When saving the dataset the unscaled burned area is classified as a 2D variable instead of a Geo2D variable\n",
    "\n",
    ":param: None\n",
    ":return: None\n",
    "\"\"\"\n",
    "def upscale_data_type_x():\n",
    "    for file in files:\n",
    "        try:\n",
    "            with Dataset(file) as netcdf_dataset:\n",
    "                # dataset containing all xarray data array (used to create the final netcdf file)\n",
    "                dataset_dict = {}\n",
    "                \n",
    "                # obtain the grid cell area value (allows for the burned area to account for the shape of the earth)\n",
    "                grid_cell_area_value = netcdf_dataset.groups[\"ancill\"].variables[\"grid_cell_area\"][:]\n",
    "                \n",
    "                # loop through every burned area month\n",
    "                for group in netcdf_dataset.groups[\"burned_area\"].groups:\n",
    "                    #obtain the current burned area group\n",
    "                    burned_area_group = netcdf_dataset.groups[\"burned_area\"].groups[group]\n",
    "                    \n",
    "                    # obtain the burned_area fraction array for the current month/group we are in\n",
    "                    burned_area_fraction = burned_area_group.variables[\"burned_fraction\"]\n",
    "                    burned_area_fraction_value = burned_area_fraction[:]\n",
    "                    \n",
    "                    # multiplying the grid cell area by the burned fraction value\n",
    "                    burned_fraction_product = grid_cell_area_value * burned_area_fraction_value\n",
    "                    burned_fraction_product = np.asarray(burned_fraction_product)\n",
    "                                            \n",
    "                    # upscale the burned fraction\n",
    "                    burned_fraction_upscaled = upscale_matrix_numpy(burned_fraction_product)\n",
    "                    \n",
    "                    # Total of orig resolution after multiplying by gridcell area should be equal to total of final (target) resolution. Both are in m^2.\n",
    "                    if evaluate_upscale_sum(burned_fraction_product, burned_fraction_upscaled):\n",
    "                        burnded_area_attribute_dict = {}\n",
    "                        \n",
    "                        # Copy attributes of the burned area fraction\n",
    "                        for attr_name in burned_area_fraction.ncattrs():\n",
    "                            burnded_area_attribute_dict[attr_name] = getattr(burned_area_fraction, attr_name)\n",
    "                            \n",
    "                        # update the units to match the upscaling process\n",
    "                        burnded_area_attribute_dict[\"units\"] = \"m^2\"\n",
    "                        \n",
    "                        # obtain the height and width from the upscale shape \n",
    "                        # create an evenly spaced array representing the longitude and the latitude\n",
    "                        height, width = burned_fraction_upscaled.shape\n",
    "                        latitudes = np.linspace(-90, 90, height)  \n",
    "                        longitudes = np.linspace(-180, 180, width) \n",
    "                        \n",
    "                        #plots the burned area before and after the rescale\n",
    "                        plot_geodata(burned_fraction_product, burned_fraction_upscaled, longitudes, latitudes)\n",
    "                        # flip the data matrix (upside down due to the GFED dataset's orientation)\n",
    "                        burned_fraction_upscaled = np.flip(burned_fraction_upscaled, 0)\n",
    "                        \n",
    "                        # create the xarray data array for the upscaled burned area and add it to the dictionary\n",
    "                        burned_area_data_array = xarray.DataArray(burned_fraction_upscaled, coords={'latitude': latitudes, 'longitude': longitudes}, dims=['latitude', 'longitude'], attrs=burnded_area_attribute_dict)\n",
    "                        dataset_dict[f\"burned_areas_{group}\"] = burned_area_data_array\n",
    "\n",
    "                # saves xarray dataset to a file\n",
    "                save_file(file, xarray.Dataset(dataset_dict))\n",
    "        except Exception as error:\n",
    "            print(\"[-] Failed to parse dataset: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_variables(netcdf_dataset):\n",
    "    # obtains the variable names that we care about from the current netcdf dataset\n",
    "    files_gfed5_variable_names = [var_name for var_name in netcdf_dataset.variables.keys() if (var_name in gfed5_variable_names)]\n",
    "    if set(files_gfed5_variable_names) == set(gfed5_variable_names):\n",
    "        files_gfed5_variable_names.append(\"Nat\")\n",
    "    return files_gfed5_variable_names\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_data_type_y() -> None:\n",
    "    \"\"\"\n",
    "    loops through each file in the classes files list Regridding (upscaling) datasets from a fine resolution to a coarse (ModelE) resolution\n",
    "    Note - This is focused on the burned area dataset and uses both netcdf (parsing/reading) and xarray (saving the data)\n",
    "        Issue (SOLVED) - When saving the dataset the unscaled burned area is classified as a 2D variable instead of a Geo2D variable\n",
    "\n",
    "    :param: None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "            with Dataset(file) as netcdf_dataset:\n",
    "                try:\n",
    "                    # dataset containing all xarray data array (used to create the final netcdf file)\n",
    "                    dataset_dict = {}\n",
    "                    # obtains the variable names that we care about from the current netcdf dataset\n",
    "                    files_gfed5_variable_names = obtain_variables(netcdf_dataset)\n",
    "                    for variable_name in files_gfed5_variable_names:\n",
    "                        match variable_name:\n",
    "                            # calculates the Nat array\n",
    "                            case \"Nat\":\n",
    "                                # transform the arrays dimensions to (720, 1440) and convert (km^2 -> m^2)\n",
    "                                # obtain all needed data array\n",
    "                                var_total_data_array = netcdf_dataset.variables[\n",
    "                                    \"Total\"\n",
    "                                ][:][0] * (10**6)\n",
    "                                var_crop_data_array = netcdf_dataset.variables[\"Crop\"][\n",
    "                                    :\n",
    "                                ][0] * (10**6)\n",
    "                                var_defo_data_array = netcdf_dataset.variables[\"Defo\"][\n",
    "                                    :\n",
    "                                ][0] * (10**6)\n",
    "                                var_peat_data_array = netcdf_dataset.variables[\"Peat\"][\n",
    "                                    :\n",
    "                                ][0] * (10**6)\n",
    "                                # calculate the Nat numpy array\n",
    "                                # equation: Total - (Crop + Defo + Peat)\n",
    "                                var_data_array = var_total_data_array - (\n",
    "                                    var_crop_data_array\n",
    "                                    + var_defo_data_array\n",
    "                                    + var_peat_data_array\n",
    "                                )\n",
    "                            # base case\n",
    "                            case _:\n",
    "                                # obtain the variables in the netcdf_dataset\n",
    "                                # dimensions (1, 720, 1440)\n",
    "                                var_data = netcdf_dataset.variables[variable_name]\n",
    "\n",
    "                                # obtain the numpy array for each netcdf variable\n",
    "                                # transform the arrays dimensions to (720, 1440) and convert the metric to km^2 -> m^2\n",
    "                                var_data_array = var_data[:][0] * (10**6)\n",
    "\n",
    "                        # preform resampling/upscaling \n",
    "                        # Conversion (720, 1440) -> (90, 144)\n",
    "                        upscaled_var_data_array = upscale_matrix_restario(\n",
    "                            var_data_array, dest_dimensions=(90, 144)\n",
    "                        )\n",
    "                        \n",
    "                        attribute_dict = {}\n",
    "\n",
    "                        # Copy attributes of the burned area fraction\n",
    "                        for attr_name in var_data.ncattrs():\n",
    "                            attribute_dict[attr_name] = getattr(var_data, attr_name)\n",
    "\n",
    "                        # update the units to match the upscaling process\n",
    "                        attribute_dict[\"units\"] = \"m^2\"\n",
    "\n",
    "                        # obtain the height and width from the upscale shape\n",
    "                        # create an evenly spaced array representing the longitude and the latitude\n",
    "                        height, width = upscaled_var_data_array.shape\n",
    "                        latitudes = np.linspace(-90, 90, height)\n",
    "                        longitudes = np.linspace(-180, 180, width)\n",
    "\n",
    "                        #plots the burned area before and after the rescale\n",
    "                        plot_geodata(var_data_array, upscaled_var_data_array, longitudes, latitudes)\n",
    "\n",
    "                        # create the xarray data array for the upscaled burned area and add it to the dictionary\n",
    "                        burned_area_data_array = xarray.DataArray(\n",
    "                            upscaled_var_data_array,\n",
    "                            coords={\"latitude\": latitudes, \"longitude\": longitudes},\n",
    "                            dims=[\"latitude\", \"longitude\"],\n",
    "                            attrs=attribute_dict,\n",
    "                        )\n",
    "                        dataset_dict[variable_name] = burned_area_data_array\n",
    "                    # saves xarray dataset to a file\n",
    "                    save_file(file, xarray.Dataset(dataset_dict))\n",
    "                except Exception as error:\n",
    "                    print(\"[-] Failed to parse dataset: \", error)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Failed to parse dataset:  create_geotiff_file() missing 1 required positional argument: 'longitude_arr'\n",
      "[-] Failed to parse dataset:  create_geotiff_file() missing 1 required positional argument: 'longitude_arr'\n",
      "[-] Failed to parse dataset:  create_geotiff_file() missing 1 required positional argument: 'longitude_arr'\n",
      "[-] Failed to parse dataset:  create_geotiff_file() missing 1 required positional argument: 'longitude_arr'\n"
     ]
    }
   ],
   "source": [
    "#upscale_data_type_x()\n",
    "upscale_data_type_y()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
